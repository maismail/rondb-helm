# Copyright (c) 2024-2024 Hopsworks AB. All rights reserved.

{{ if .Values.restoreFromBackup.backupId -}}
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ include "rondb.restoreNativeBackupJobname" . }}
  namespace: {{ .Release.Namespace }}
spec:
  backoffLimit: 2
  parallelism: 1
  template:
    metadata:
      labels:
        rondbService: {{ include "rondb.labels.rondbService.restore-backup" $ }}
    spec:
      restartPolicy: Never
      serviceAccountName: rondb-backups-sa
{{- include "rondb.PodSecurityContext" $ | indent 6 }}
{{- include "rondb.imagePullSecrets" . | indent 6 }}
{{- include "rondb.nodeSelector" (dict "nodeSelector" $.Values.nodeSelector.backup) | indent 6 }}
{{- include "rondb.tolerations" (dict "tolerations" $.Values.tolerations.backup) | indent 6 }}
{{- if .Values.priorityClass }}
      priorityClassName: {{ .Values.priorityClass | quote }}
{{- end }}
      initContainers:
{{/*
    1. Wait for data nodes to be up
    2. Start up a MySQLd to create MySQL system tables; shut it down again
    3. One ndbd runs restore-meta and disable-indexes
    4. All ndbds run restore-data
    5. One ndbd runs rebuild-indexes

    The metadata is globally replicated, hence we only need to restore it on a single node.
    The same metadata will be in all backup parts.

    The `--no-binlog` parameter is used to avoid writing the backup to the binary log.
    (I assume) This means that they will not create any events in the NDB API.

    Also note that following this, there will be a MySQLd Job spinning up to restore
    MySQL metadata (users and databases).
*/}}
{{ include "rondb.container.waitDatanodes" . | indent 6 }}
      - name: initialize-mysqld-once
        image: {{ include "image_address" (dict "image" .Values.images.rondb) }}
        imagePullPolicy: {{ $.Values.imagePullPolicy }}
{{ include "rondb.ContainerSecurityContext" $ | indent 8 }}
        command:
        - /bin/bash
        - -c
        - |
          set -e

          SOCKET={{ include "rondb.dataDir" $ }}/mysql.sock
          CMD=("mysqld" \
            "--ndbcluster" \
            "--ndb-connectstring={{ include "rondb.mgmdHostname" . }}:1186" \
            "--ndb-cluster-connection-pool=1" \
            "--user=mysql" \
            "--basedir=/srv/hops/mysql" \
            "--datadir={{ include "rondb.mysqldDataDir" . }}" \
            "--log-error-verbosity=3" \
            "--socket=$SOCKET")

          (set -x; "${CMD[@]}" \
            --initialize-insecure \
            --explicit_defaults_for_timestamp)
        
          echo && echo "[Entrypoint] Successfully initialized MySQLd" && echo

          (set -x; "${CMD[@]}" \
            --skip-networking \
            --daemonize)

          until mysqladmin -uroot --socket="$SOCKET" ping --silent --connect-timeout=2; do
            echo "[K8s Entrypoint MySQLd] Failed pinging MySQLd on attempt $attempt" && sleep 1
            attempt=$((attempt+1))
            if [[ $attempt -gt 5000 ]]; then
                echo "[K8s Entrypoint MySQLd] Failed pinging MySQLd after 30 attempts" && exit 1
            fi
          done

          echo && echo "[K8s Entryoint MySQLd] MySQLd is up and running" && echo

          mysql -uroot --protocol=socket --socket="$SOCKET" -e "SHOW DATABASES;"

          echo && echo "[K8s Entryoint MySQLd] Killing the MySQLd now" && echo

          mysqladmin -uroot -p"" --silent shutdown --socket="$SOCKET"
        resources:
          limits:
            cpu: {{ .Values.resources.limits.cpus.mysqlds }}
            memory: {{ .Values.resources.limits.memory.mysqldMiB }}Mi
          requests:
            cpu: {{ .Values.resources.requests.cpus.mysqlds }}
            memory: {{ .Values.resources.requests.memory.mysqldMiB }}Mi
      - name: restore-ndbmtd-metadata
        image: {{ include "image_address" (dict "image" $.Values.images.toolbox) }}
        imagePullPolicy: {{ $.Values.imagePullPolicy }}
        command:
        - /bin/bash
        - -c
        - |
{{- $allDbsToExclude := concat 
    .Values.restoreFromBackup.excludeDatabases
    (include "rondb.databases.benchmarking" . | fromYamlArray)
    ( include "rondb.databases.heartbeat" . | list)
}}
{{- $excludeDbsCsv := include "rondb.arrayToCsv" (dict "array" $allDbsToExclude) }}
{{- $excludeDbs := "" }}
{{- if $excludeDbsCsv }}
    {{- $excludeDbs = printf "--exclude-databases=%s" $excludeDbsCsv }}
{{- end }}
{{- $excludeTablesCsv := include "rondb.arrayToCsv" (dict "array" .Values.restoreFromBackup.excludeTables) }}
{{- $excludeTables := "" }}
{{- if $excludeTablesCsv }}
    {{- $excludeTables = printf "--exclude-tables=%s" $excludeTablesCsv }}
{{- end }}

          set -e

          # Metadata is fully replicated, hence we only need to run it on one directory
          # Don't take over the old disk *structure*
          NODE_ID=1
          RUN_CMD="ndb_restore \
            --restore-meta \
            --disable-indexes \
            --no-restore-disk-objects \
            --no-binlog \
            --ndb-connectstring=$MGMD_HOST:1186 \
            --nodeid=$NODE_ID \
            --backupid=$BACKUP_ID \
            --backup-path={{ include "rondb.ndbmtd.backupDataDir" $ }}/BACKUP/BACKUP-$BACKUP_ID/$NODE_ID \
            {{ $excludeDbs }} \
            {{ $excludeTables }}"

          (set -x; kubectl exec \
            "node-group-0-0" \
            -c ndbmtd \
            -n {{ .Release.Namespace }} \
            -- /bin/bash -c "$RUN_CMD")

          echo "Successfully restored RonDB native metadata"
        env:
        - name: MGMD_HOST
          value: {{ include "rondb.mgmdHostname" . }}
        - name: BACKUP_ID
          value: {{ .Values.restoreFromBackup.backupId | quote }}
        resources:
          limits:
            cpu: {{ .Values.resources.limits.cpus.restore }}
            memory: 200Mi
      - name: restore-ndbmtd-data
        image: {{ include "image_address" (dict "image" $.Values.images.toolbox) }}
        imagePullPolicy: {{ $.Values.imagePullPolicy }}
        command:
        - /bin/bash
        - -c
        - |
          set -e

{{ include "rondb.mapNewNodesToBackedUpNodes" . | indent 10 }}

          NUM_NODE_GROUPS={{ .Values.clusterSize.numNodeGroups }}
          NUM_REPLICAS={{ .Values.clusterSize.activeDataReplicas }}

          for ((g = 0; g < NUM_NODE_GROUPS; g++)); do
            for ((r = 0; r < NUM_REPLICAS; r++)); do

              DATANODE_PODNAME="node-group-$g-$r"
              NODE_ID_OFFSET=$(($g*3))
              NEW_NODE_ID=$(($NODE_ID_OFFSET+$r+1))

              BACKUP_NODE_IDS=${MAP_NODE_IDS[$NEW_NODE_ID]}
              for BACKUP_NODE_ID in $BACKUP_NODE_IDS; do

                RUN_CMD="ndb_restore \
                  --restore-data \
                  --no-binlog \
                  --allow-unique-indexes \
                  --ndb-connectstring=$MGMD_HOST:1186 \
                  --nodeid=$BACKUP_NODE_ID \
                  --backupid=$BACKUP_ID \
                  --backup-path={{ include "rondb.ndbmtd.backupDataDir" $ }}/BACKUP/BACKUP-$BACKUP_ID/$BACKUP_NODE_ID \
                  {{ $excludeDbs }} \
                  {{ $excludeTables }}"

                (set -x; kubectl exec \
                  "$DATANODE_PODNAME" \
                  -c ndbmtd \
                  -n {{ .Release.Namespace }} \
                  -- /bin/bash -c "$RUN_CMD") &

              done
            done
          done
          wait 
          echo "Successfully restored RonDB native data"
        env:
        - name: MGMD_HOST
          value: {{ include "rondb.mgmdHostname" . }}
        - name: BACKUP_ID
          value: {{ .Values.restoreFromBackup.backupId | quote }}
{{- if eq $.Values.restoreFromBackup.objectStorageProvider "s3" }}
        - name: ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
{{- toYaml $.Values.restoreFromBackup.s3.keyCredentialsSecret | nindent 14 }}
              optional: true
        - name: SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
{{- toYaml $.Values.restoreFromBackup.s3.secretCredentialsSecret | nindent 14 }}
              optional: true
{{- end }}
        - name: RCLONE_MOUNT_FILEPATH
          value: {{ include "rondb.rawRCloneConf" $ }}
        # This will be read by rclone
        - name: RCLONE_CONFIG
          value: /home/hopsworks/rclone.conf
        volumeMounts:
        - name: rclone-configs
          mountPath: {{ include "rondb.rawRCloneConf" $ }}
          subPath: rclone.conf
        resources:
          limits:
            cpu: {{ .Values.resources.limits.cpus.restore }}
            memory: 200Mi
{{/*
    The `--restore-epoch` parameter is used to add a row to the mysql.ndb_apply_status table:

    mysql> select * from ndb_apply_status;
    +-----------+--------------+----------+-----------+---------+
    | server_id | epoch        | log_name | start_pos | end_pos |
    +-----------+--------------+----------+-----------+---------+
    |         0 | 498216206335 |          |         0 |       0 |
    +-----------+--------------+----------+-----------+---------+

    Note that server_id is always 0, and the epoch is the epoch of the backup.

    TODO: Figure out whether it is an issue to ALWAYS use `--restore-epoch` during restore.
*/}}
      - name: restore-backup-indexes
        image: {{ include "image_address" (dict "image" $.Values.images.toolbox) }}
        imagePullPolicy: {{ $.Values.imagePullPolicy }}
        command:
        - /bin/bash
        - -c
        - |

          set -e

          RUN_CMD="ndb_restore \
            --rebuild-indexes \
            --no-binlog \
            --restore-epoch \
            --ndb-connectstring=$MGMD_HOST:1186 \
            --nodeid=1 \
            --backupid=$BACKUP_ID \
            --backup-path={{ include "rondb.ndbmtd.backupDataDir" $ }}/BACKUP/BACKUP-$BACKUP_ID/1 \
            {{ $excludeDbs }} \
            {{ $excludeTables }}"

          (set -x; kubectl exec \
            "node-group-0-0" \
            -c ndbmtd \
            -n {{ .Release.Namespace }} \
            -- /bin/bash -c "$RUN_CMD")

          echo "Successfully rebuilt RonDB indexes"
        env:
        - name: MGMD_HOST
          value: {{ include "rondb.mgmdHostname" . }}
        - name: BACKUP_ID
          value: {{ .Values.restoreFromBackup.backupId | quote }}
        resources:
          limits:
            cpu: {{ .Values.resources.limits.cpus.restore }}
            memory: 200Mi
      containers:
      - name: remove-native-backup
        image: {{ include "image_address" (dict "image" $.Values.images.toolbox) }}
        imagePullPolicy: {{ $.Values.imagePullPolicy }}
        command:
        - /bin/bash
        - -c
        - |

          set -e

          NUM_NODE_GROUPS={{ .Values.clusterSize.numNodeGroups }}
          NUM_REPLICAS={{ .Values.clusterSize.activeDataReplicas }}

          for ((g = 0; g < NUM_NODE_GROUPS; g++)); do
            for ((r = 0; r < NUM_REPLICAS; r++)); do

              DATANODE_PODNAME="node-group-$g-$r"
              GENERAL_BACKUP_DIR="{{ include "rondb.ndbmtd.backupDataDir" $ }}/BACKUP"
              BACKUP_DIR="$GENERAL_BACKUP_DIR/BACKUP-$BACKUP_ID"
              (set -x; kubectl exec \
                "$DATANODE_PODNAME" \
                -c ndbmtd \
                -n {{ .Release.Namespace }} \
                -- /bin/bash -c "rm -rf $BACKUP_DIR && ls -la $GENERAL_BACKUP_DIR || true")

            done
          done

          echo "Successfully removed native backup ID $BACKUP_ID from data node Pods"
        env:
        - name: BACKUP_ID
          value: {{ .Values.restoreFromBackup.backupId | quote }}
        resources:
          limits:
            cpu: {{ .Values.resources.limits.cpus.restore }}
            memory: 200Mi
      volumes:
      - name: rclone-configs
        configMap:
          name: rclone-configs
{{- end }}
