# Copyright (c) 2024-2024 Hopsworks AB. All rights reserved.

{{ range $nodeGroup := until ($.Values.clusterSize.numNodeGroups | int) }}
apiVersion: apps/v1
kind: StatefulSet
metadata:
  # This will have the form of "node-group-0-0", "node-group-0-1", etc.
  name: node-group-{{ $nodeGroup }}
  namespace: {{ $.Release.Namespace }}
spec:
  serviceName: ndbmtd-ng-{{ $nodeGroup }}
  # When using a startup probe, one ndbmtd is only started after
  # the other is fully connected. However, when starting up a
  # cluster (using flag --initial), the MGMd will wait for all
  # data nodes to start at the same time.
  podManagementPolicy: Parallel
  replicas: {{ $.Values.clusterSize.activeDataReplicas }}
{{- if lt ($.Values.clusterSize.activeDataReplicas | int) 2 }}
  # Don't kill a data node if we only have 1 replica.
  # Even when we start with 2 replicas and scale down to 1,
  # whilst also scaling our replicas vertically, K8s
  # will first kill the second replica before exchanging our
  # first replica. In short: even if we start with
  # 2 replicas, we risk downtime when vertically
  # scaling.
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      partition: 1
{{- end }}
  selector:
    # Used by the Deployment to select and manage existing pods with the specified label
    matchLabels:
      rondbService: {{ include "rondb.labels.rondbService.ndbmtd" $ }}
      nodeGroup: {{ $nodeGroup | quote }}
  # StatefulSets work with PVCs to create a dedicated persistent volume for
  # each pod replica, ensuring that a pod always re-attaches to the same data
  # even if it is rescheduled to a different node.
  volumeClaimTemplates:
    - metadata:
        name: rondb-ndbmtd
      spec:
        accessModes: [ReadWriteOnce]
{{ include "rondb.storageClass.default" $ | indent 8 }}
{{- $backupGiB := add
    (div $.Values.resources.limits.memory.ndbmtdsMiB 1024)
    $.Values.resources.requests.storage.undoLogsGiB
}}
        resources:
          requests:
{{- if not $.Values.resources.requests.storage.classes.diskColumns }}
            storage: {{ add
                $backupGiB
                $.Values.resources.requests.storage.diskColumnGiB
                $.Values.resources.requests.storage.redoLogGiB
                $.Values.resources.requests.storage.undoLogsGiB
                $.Values.resources.requests.storage.slackGiB
                $.Values.resources.requests.storage.logGiB
            }}Gi
{{- else }}
            storage: {{ add
                $backupGiB
                $.Values.resources.requests.storage.redoLogGiB
                $.Values.resources.requests.storage.undoLogsGiB
                $.Values.resources.requests.storage.slackGiB
                $.Values.resources.requests.storage.logGiB
            }}Gi
    - metadata:
        name: rondb-ndbmtd-dc
      spec:
        accessModes: [ReadWriteOnce]
{{ include "rondb.storageClass.diskColumns" $ | indent 8 }}
        resources:
          requests:
            storage: {{ add
                $.Values.resources.requests.storage.diskColumnGiB 
                $.Values.resources.requests.storage.slackGiB
            }}Gi
{{- end }}
  # Still in beta mode (Jan 2024)
  persistentVolumeClaimRetentionPolicy:
    whenDeleted: Delete
    whenScaled: Retain
  template:
    metadata:
      # Used to apply labels to all pods created by the StatefulSet
      labels:
        rondbService: {{ include "rondb.labels.rondbService.ndbmtd" $ }}
        nodeGroup: {{ $nodeGroup | quote }}
      # This is an easy way to restart data nodes without changing specs.
      annotations:
{{- range $k, $v := $.Values.meta.ndbmtd.statefulSet.podAnnotations }}
        {{ $k | quote }}: {{ $v | quote }}
{{- end }}
    spec:
{{- include "rondb.imagePullSecrets" $ | indent 6 }}
{{- include "rondb.PodSecurityContext" $ | indent 6 }}
{{- include "rondb.nodeSelector" (dict "nodeSelector" $.Values.nodeSelector.ndbmtd) | indent 6 }}
{{- include "rondb.tolerations" (dict "tolerations" $.Values.tolerations.ndbmtd) | indent 6 }}
      topologySpreadConstraints:
      # Spread across zones
      - maxSkew: 1
        topologyKey: topology.kubernetes.io/zone
        # If the nodes don't have a zone label, and we force this, the pods won't be scheduled
        whenUnsatisfiable: ScheduleAnyway
        nodeAffinityPolicy: Honor
        nodeTaintsPolicy: Honor
        labelSelector:
          matchLabels:
            nodeGroup: {{ $nodeGroup | quote }}
      affinity:
        podAntiAffinity:
{{- if $.Values.isMultiNodeCluster }}
          requiredDuringSchedulingIgnoredDuringExecution:
          # Replicas of same node group *must* be on different nodes
          - labelSelector:
              matchExpressions:
                - key: nodeGroup
                  operator: In
                  values:
                  - {{ $nodeGroup | quote }}
            topologyKey: kubernetes.io/hostname
          # Data nodes *must* be on different nodes than MGMds
          # When having e.g. 3 nodes, it is not worth having 3 replicas. This
          # won't be scheduled.
          # Example, we have:
          # - a replication factor of 2
          # - 1 node group for simplicity
          # - one replica on the same node as the MGMd and this node goes down
          # Then the data node on the other node would not be able to
          # use the MGMd for arbitration and would then also go down.
          - labelSelector:
              matchExpressions:
                - key: rondbService
                  operator: In
                  values:
                  - mgmd
            topologyKey: kubernetes.io/hostname
{{- end }}
          # Can be in different zones
          preferredDuringSchedulingIgnoredDuringExecution:
          # Placing a MGMd into a different AZ will not affect performance at all,
          # but greatly benefit the availability of the cluster.
          - weight: 100
            podAffinityTerm:
              labelSelector:
                matchExpressions:
                - key: rondbService
                  operator: In
                  values:
                  - mgmd
              topologyKey: topology.kubernetes.io/zone
{{- if $.Values.priorityClass }}
      priorityClassName: {{ $.Values.priorityClass | quote }}
{{- end  }}
      terminationGracePeriodSeconds: {{ $.Values.terminationGracePeriodSeconds }}
      initContainers:
{{/*
    Placing this here speeds up the process of restoring from a backup.
    However, if the data node is deleted, this will be triggered again.
    We avoid downloading again, by checking the data node filesystem.
    However, this can be difficult to maintain.
    
    TODO: Since we already have the rclone-listener sidecar, use it to also
        download the backup-to-be-restored.
*/}}
{{- if $.Values.restoreFromBackup.backupId }}
      - name: download-backup-to-restore
        image: {{ include "image_address" (dict "image" $.Values.images.toolbox) }}
        # Setting resources in order to be in QoS class Guaranteed
        resources:
          limits:
{{- if $.Values.staticCpuManagerPolicy }}
            # cpu *has to* be set to 1, otherwise the QoS class will be Burstable
            cpu: 1
{{- else }}
            cpu: 0.2
{{- end }}
            memory: 200Mi
        command:
        - /bin/bash
        - -c
        - |
{{ tpl ($.Files.Get "files/scripts/backups/native_download.sh") $ | indent 10 }}
        env:
        - name: NODE_GROUP
          value: {{ $nodeGroup | quote }}
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        - name: FILE_SYSTEM_PATH
          value: /home/hopsworks/data/ndb_data
        - name: BACKUP_ID
          value: {{ $.Values.restoreFromBackup.backupId | quote }}
{{- if eq $.Values.restoreFromBackup.objectStorageProvider "s3" }}
        - name: ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
{{- toYaml $.Values.restoreFromBackup.s3.keyCredentialsSecret | nindent 14 }}
              optional: true
        - name: SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
{{- toYaml $.Values.restoreFromBackup.s3.secretCredentialsSecret | nindent 14 }}
              optional: true
{{- end }}
        - name: RCLONE_MOUNT_FILEPATH
          value: {{ include "rondb.rawRCloneConf" $ }}
        # This will be read by rclone
        - name: RCLONE_CONFIG
          value: /home/hopsworks/rclone.conf
        volumeMounts:
        - name: rondb-ndbmtd
          mountPath: /home/hopsworks/data
        - name: rclone-configs
          mountPath: {{ include "rondb.rawRCloneConf" $ }}
          subPath: rclone.conf
{{- end }}
      - name: mgmd-dependency-check
        image: {{ include "image_address" (dict "image" $.Values.images.rondb) }}
        imagePullPolicy: {{ $.Values.imagePullPolicy }}
{{ include "rondb.ContainerSecurityContext" $ | indent 8 }}
        # Setting resources in order to be in QoS class Guaranteed
        resources:
          limits:
{{- if $.Values.staticCpuManagerPolicy }}
            # cpu *has to* be set to 1, otherwise the QoS class will be Burstable
            cpu: 1
{{- else }}
            cpu: 0.2
{{- end }}
            memory: 200Mi
        env:
        - name: MGMD_HOST
          value: {{ include "rondb.mgmdHostname" $ }}
        - name: NODE_GROUP
          value: {{ $nodeGroup | quote }}
        - name: POD_NAME
          valueFrom:
            fieldRef:
              fieldPath: metadata.name
        command:
        - /bin/bash
        - -c
        - |
          until nslookup $MGMD_HOST; do
            echo "Waiting for $MGMD_HOST to be resolvable..."
            sleep $(((RANDOM % 2)+2))
          done

          # This is necessary for MGMd to accept the data node.
          # Placing this in initContainer because a main-container restart
          # will not change the Pod's IP address.
{{ include "rondb.resolveOwnIp" $ | indent 10 }}

          # Since the main container *deactivates* the nodes, it is important
          # that the main container also activates the nodes. This is because StatefulSets
          # will always try to restart a container (if it fails) and not the entire Pod.
          # Hence, this is just placed here to accelerate the activation in main container
          # so that the startupProbe is more accurate. The command is idempotent.
          MGM_CONNECTSTRING=$MGMD_HOST:1186

{{ include "rondb.nodeId" $ | indent 10 }}

          echo "[K8s Entrypoint ndbmtd] Activating node id $NODE_ID via MGM client"
          while ! ndb_mgm --ndb-connectstring="$MGM_CONNECTSTRING" --connect-retries=1 -e "$NODE_ID activate"; do
            echo "[K8s Entrypoint ndbmtd] Activation failed. Retrying..." >&2
            sleep $((NODE_GROUP + 2))
          done
          echo "[K8s Entrypoint ndbmtd] Activated node id $NODE_ID via MGM client"
      containers:
      - name: ndbmtd
        image: {{ include "image_address" (dict "image" $.Values.images.rondb) }}
        imagePullPolicy: {{ $.Values.imagePullPolicy }}
{{ include "rondb.ContainerSecurityContext" $ | indent 8 }}
        command:
        - /bin/bash
        - -c
        - |
{{ tpl ($.Files.Get "files/scripts/ndbmtds.sh") $ | indent 10 }}
        ports:
          - containerPort: 11860
        resources:
          # If not specified, requested resources are set to the limits.
          # This is important to acquire the QoS class Guaranteed.
          limits:
            cpu: {{ $.Values.resources.limits.cpus.ndbmtds }}
            memory: {{ $.Values.resources.limits.memory.ndbmtdsMiB }}Mi
        env:
          - name: MGM_CONNECTION_STRING
            value: {{ include "rondb.mgmdHostname" $ }}:1186
          - name: MGMD_HOST
            value: {{ include "rondb.mgmdHostname" $ }}
          - name: NODE_GROUP
            value: {{ $nodeGroup | quote }}
          - name: POD_NAME
            valueFrom:
              fieldRef:
                fieldPath: metadata.name
          - name: FILE_SYSTEM_PATH
            value: {{ include "rondb.ndbmtd.fileSystemPath" $ }}
        startupProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - |
{{ include "rondb.nodeId" $ | indent 14 }}
              ./docker/rondb_standalone/healthcheck.sh $MGM_CONNECTION_STRING $NODE_ID
          initialDelaySeconds: 15
          failureThreshold: 150
          periodSeconds:  10
          timeoutSeconds: 10
        # This has to account for a failing MGMd as well.
        # There is no point in killing the data node
        # if the MGMd is not up / ready.
        livenessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - |
              set +e
              nslookup $MGMD_HOST > /dev/null 2>&1
              NS_LOOKUP_EXIT_CODE=$!
              if [ $NS_LOOKUP_EXIT_CODE -ne 0 ]; then
                  echo "MGMd at '$MGMD_HOST' is not ready, we cannot check whether the data node is connected"
                  exit 0
              fi
              set -e
{{ include "rondb.nodeId" $ | indent 14 }}
              ./docker/rondb_standalone/healthcheck.sh $MGM_CONNECTION_STRING $NODE_ID
          initialDelaySeconds: 5
          failureThreshold: 10
          periodSeconds:  10
          timeoutSeconds: 10
        # Adding this reduces the likelihood of continuing with a
        # rolling update when something is off. We might not want
        # to restart the data node just yet, but we want to flash
        # some warning signals. API nodes will still be able to connect
        # if the data nodes are not ready since publishNotReadyAddresses
        # is set to True.
        readinessProbe:
          exec:
            command:
            - /bin/bash
            - -c
            - |
{{ include "rondb.nodeId" $ | indent 14 }}
              ./docker/rondb_standalone/healthcheck.sh $MGM_CONNECTION_STRING $NODE_ID
          initialDelaySeconds: 5
          failureThreshold: 3
          periodSeconds:  5
          timeoutSeconds: 5
        volumeMounts:
        - name: rondb-ndbmtd
          mountPath: {{ include "rondb.ndbmtd.volumeSymlink" $ }}
{{- if $.Values.resources.requests.storage.classes.diskColumns }}
        - name: rondb-ndbmtd-dc
          mountPath: {{ include "rondb.ndbmtd.fileSystemPathDataFiles" $ }}
{{- end }}
{{- if $.Values.backups.enabled -}}
{{/* This container is used to receive calls to push backups to object storage */}}
      - name: rclone-listener
        image: {{ include "image_address" (dict "image" $.Values.images.toolbox) }}
        imagePullPolicy: {{ $.Values.imagePullPolicy }}
{{ include "rondb.ContainerSecurityContext" $ | indent 8 }}
        command:
        - /bin/bash
        - -c
        - |
          set -e
{{ include "rondb.createRcloneConfig" $ | indent 10 }}
          sleep infinity
        resources:
          # If not specified, requested resources are set to the limits.
          # This is important to acquire the QoS class Guaranteed.
          limits:
{{- if $.Values.staticCpuManagerPolicy }}
            # cpu *has to* be set to 1, otherwise the QoS class will be Burstable
            cpu: 1
{{- else }}
            cpu: 0.3
{{- end }}
            memory: 300Mi
        env:
{{- if eq $.Values.backups.objectStorageProvider "s3" }}
        - name: ACCESS_KEY_ID
          valueFrom:
            secretKeyRef:
{{- toYaml $.Values.backups.s3.keyCredentialsSecret | nindent 14 }}
              optional: true
        - name: SECRET_ACCESS_KEY
          valueFrom:
            secretKeyRef:
{{- toYaml $.Values.backups.s3.secretCredentialsSecret | nindent 14 }}
              optional: true
{{- end }}
        - name: RCLONE_MOUNT_FILEPATH
          value: {{ include "rondb.rawRCloneConf" $ }}
        # This will be read by rclone
        - name: RCLONE_CONFIG
          value: /home/hopsworks/rclone.conf
        volumeMounts:
        - name: rondb-ndbmtd
          mountPath: /home/hopsworks/data
        - name: rclone-configs
          mountPath: {{ include "rondb.rawRCloneConf" $ }}
          subPath: rclone.conf
{{- end }}
{{- if or $.Values.backups.enabled $.Values.restoreFromBackup.backupId }}
      volumes:
      - name: rclone-configs
        configMap:
          name: rclone-configs
{{- end }}
---
apiVersion: v1
kind: Service
metadata:
  # Match the spec.serviceName
  name: ndbmtd-ng-{{ $nodeGroup }}
  namespace: {{ $.Release.Namespace }}
spec:
  # Headless service for individual DNS records for the pods
  clusterIP: None
  # So we do not rely on the readiness probe to connect to the MGMd
  publishNotReadyAddresses: true
  # Match the spec.template.metadata.labels of the StatefulSet
  selector:
    rondbService: {{ include "rondb.labels.rondbService.ndbmtd" $ }}
    nodeGroup: {{ $nodeGroup | quote }}
  ports:
    - protocol: TCP
      port: 11860
      targetPort: 11860
---
{{ end }}
